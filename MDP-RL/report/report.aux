\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Problems}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Grid World}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Block Dude}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Markov Decision Processes}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Value and Policy Iteration - Grid World}{3}{subsection.3.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:GW:size}{{1a}{3}{Number of iterations according to the size of the grid.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:GW:size}{{a}{3}{Number of iterations according to the size of the grid.\relax }{figure.caption.3}{}}
\newlabel{fig:GW:probability}{{1b}{3}{Number of iterations according to the stochastic success probability of the actions.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:GW:probability}{{b}{3}{Number of iterations according to the stochastic success probability of the actions.\relax }{figure.caption.3}{}}
\newlabel{fig:GW:obstacles}{{1c}{3}{Number of iterations according to the percentage of obstacles in the grid.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:GW:obstacles}{{c}{3}{Number of iterations according to the percentage of obstacles in the grid.\relax }{figure.caption.3}{}}
\newlabel{fig:GW:reward}{{1d}{3}{Number of iterations according to the reward value for reaching the goal.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:GW:reward}{{d}{3}{Number of iterations according to the reward value for reaching the goal.\relax }{figure.caption.3}{}}
\newlabel{fig:GW:discount}{{1e}{3}{Number of iterations according to the discount factor.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:GW:discount}{{e}{3}{Number of iterations according to the discount factor.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Number of iterations to converge to an optimal policy according to the parameters of the Grid World problem.\relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:GW:iterations}{{1}{3}{Number of iterations to converge to an optimal policy according to the parameters of the Grid World problem.\relax }{figure.caption.3}{}}
\newlabel{fig:GW:size:VI}{{2a}{4}{Value Iteration\relax }{figure.caption.5}{}}
\newlabel{sub@fig:GW:size:VI}{{a}{4}{Value Iteration\relax }{figure.caption.5}{}}
\newlabel{fig:GW:size:PI}{{2b}{4}{Policy Iteration\relax }{figure.caption.5}{}}
\newlabel{sub@fig:GW:size:PI}{{b}{4}{Policy Iteration\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Optimal policy for a 20x20 Grid World problem.\relax }}{4}{figure.caption.5}\protected@file@percent }
\newlabel{fig:GW:size:comparison}{{2}{4}{Optimal policy for a 20x20 Grid World problem.\relax }{figure.caption.5}{}}
\newlabel{fig:GW:probability:VI}{{3a}{4}{Value Iteration\relax }{figure.caption.7}{}}
\newlabel{sub@fig:GW:probability:VI}{{a}{4}{Value Iteration\relax }{figure.caption.7}{}}
\newlabel{fig:GW:probability:PI}{{3b}{4}{Policy Iteration\relax }{figure.caption.7}{}}
\newlabel{sub@fig:GW:probability:PI}{{b}{4}{Policy Iteration\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Optimal policy for a 0.6 success probability Grid World problem.\relax }}{4}{figure.caption.7}\protected@file@percent }
\newlabel{fig:GW:probability:comparison}{{3}{4}{Optimal policy for a 0.6 success probability Grid World problem.\relax }{figure.caption.7}{}}
\newlabel{fig:GW:obstacles:VI}{{4a}{5}{Value Iteration\relax }{figure.caption.9}{}}
\newlabel{sub@fig:GW:obstacles:VI}{{a}{5}{Value Iteration\relax }{figure.caption.9}{}}
\newlabel{fig:GW:obstacles:PI}{{4b}{5}{Policy Iteration\relax }{figure.caption.9}{}}
\newlabel{sub@fig:GW:obstacles:PI}{{b}{5}{Policy Iteration\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Optimal policy for a 0.2 wall appearance percent Grid World problem.\relax }}{5}{figure.caption.9}\protected@file@percent }
\newlabel{fig:GW:obstacles:comparison}{{4}{5}{Optimal policy for a 0.2 wall appearance percent Grid World problem.\relax }{figure.caption.9}{}}
\newlabel{fig:GW:reward:VI}{{5a}{5}{Value Iteration\relax }{figure.caption.11}{}}
\newlabel{sub@fig:GW:reward:VI}{{a}{5}{Value Iteration\relax }{figure.caption.11}{}}
\newlabel{fig:GW:reward:PI}{{5b}{5}{Policy Iteration\relax }{figure.caption.11}{}}
\newlabel{sub@fig:GW:reward:PI}{{b}{5}{Policy Iteration\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Optimal policy for a 3.0 reward value Grid World problem.\relax }}{5}{figure.caption.11}\protected@file@percent }
\newlabel{fig:GW:reward:comparison}{{5}{5}{Optimal policy for a 3.0 reward value Grid World problem.\relax }{figure.caption.11}{}}
\newlabel{fig:GW:discount:VI}{{6a}{6}{Value Iteration\relax }{figure.caption.13}{}}
\newlabel{sub@fig:GW:discount:VI}{{a}{6}{Value Iteration\relax }{figure.caption.13}{}}
\newlabel{fig:GW:discount:PI}{{6b}{6}{Policy Iteration\relax }{figure.caption.13}{}}
\newlabel{sub@fig:GW:discount:PI}{{b}{6}{Policy Iteration\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Optimal policy for a 0.9 discount factor Grid World problem.\relax }}{6}{figure.caption.13}\protected@file@percent }
\newlabel{fig:GW:discount:comparison}{{6}{6}{Optimal policy for a 0.9 discount factor Grid World problem.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Value and Policy Iteration - Block Dude}{6}{subsection.3.2}\protected@file@percent }
\newlabel{fig:BD:level}{{7a}{6}{Number of iterations according to the level of the game.\relax }{figure.caption.16}{}}
\newlabel{sub@fig:BD:level}{{a}{6}{Number of iterations according to the level of the game.\relax }{figure.caption.16}{}}
\newlabel{fig:BD:reward}{{7b}{6}{Number of iterations according to the reward value for reaching the goal.\relax }{figure.caption.16}{}}
\newlabel{sub@fig:BD:reward}{{b}{6}{Number of iterations according to the reward value for reaching the goal.\relax }{figure.caption.16}{}}
\newlabel{fig:BD:discount}{{7c}{6}{Number of iterations according to the discount factor.\relax }{figure.caption.16}{}}
\newlabel{sub@fig:BD:discount}{{c}{6}{Number of iterations according to the discount factor.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Number of iterations to converge to an optimal policy according to the parameters of the Grid World problem.\relax }}{6}{figure.caption.16}\protected@file@percent }
\newlabel{fig:BD:iterations}{{7}{6}{Number of iterations to converge to an optimal policy according to the parameters of the Grid World problem.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Reinforcement Learning}{7}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Q-Learning - Grid World}{7}{subsection.4.1}\protected@file@percent }
\newlabel{fig:GW:qinit}{{8a}{8}{Time according to the initial Q value.\relax }{figure.caption.22}{}}
\newlabel{sub@fig:GW:qinit}{{a}{8}{Time according to the initial Q value.\relax }{figure.caption.22}{}}
\newlabel{fig:GW:epsilon}{{8b}{8}{Time according to the epsilon value.\relax }{figure.caption.22}{}}
\newlabel{sub@fig:GW:epsilon}{{b}{8}{Time according to the epsilon value.\relax }{figure.caption.22}{}}
\newlabel{fig:GW:rate}{{8c}{8}{Time according to the the learning rate.\relax }{figure.caption.22}{}}
\newlabel{sub@fig:GW:rate}{{c}{8}{Time according to the the learning rate.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Number of iterations to converge to an optimal policy according to the parameters of the Grid World problem.\relax }}{8}{figure.caption.22}\protected@file@percent }
\newlabel{fig:GW:QL}{{8}{8}{Number of iterations to converge to an optimal policy according to the parameters of the Grid World problem.\relax }{figure.caption.22}{}}
\newlabel{fig:GW:qinit1}{{9a}{9}{Average reward for GW with initial Q value of 0.3.\relax }{figure.caption.23}{}}
\newlabel{sub@fig:GW:qinit1}{{a}{9}{Average reward for GW with initial Q value of 0.3.\relax }{figure.caption.23}{}}
\newlabel{fig:GW:qintit2}{{9b}{9}{Average reward for GW with initial Q value of 30.\relax }{figure.caption.23}{}}
\newlabel{sub@fig:GW:qintit2}{{b}{9}{Average reward for GW with initial Q value of 30.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Average reward for GW with different initial Q values.\relax }}{9}{figure.caption.23}\protected@file@percent }
\newlabel{fig:GW:aqinit}{{9}{9}{Average reward for GW with different initial Q values.\relax }{figure.caption.23}{}}
\newlabel{fig:GW:epsilon1}{{10a}{9}{Average reward for GW with epsilon value of 0.05.\relax }{figure.caption.24}{}}
\newlabel{sub@fig:GW:epsilon1}{{a}{9}{Average reward for GW with epsilon value of 0.05.\relax }{figure.caption.24}{}}
\newlabel{fig:GW:epsilon2}{{10b}{9}{Average reward for GW with epsilon value of 0.8.\relax }{figure.caption.24}{}}
\newlabel{sub@fig:GW:epsilon2}{{b}{9}{Average reward for GW with epsilon value of 0.8.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Average reward for GW with different epsilon values.\relax }}{9}{figure.caption.24}\protected@file@percent }
\newlabel{fig:GW:aepsilon}{{10}{9}{Average reward for GW with different epsilon values.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Q-Learning - Block Dude}{9}{subsection.4.2}\protected@file@percent }
\newlabel{fig:BD:qinit}{{11a}{9}{Time according to the initial Q value.\relax }{figure.caption.25}{}}
\newlabel{sub@fig:BD:qinit}{{a}{9}{Time according to the initial Q value.\relax }{figure.caption.25}{}}
\newlabel{fig:BD:epsilon}{{11b}{9}{Time according to the epsilon value.\relax }{figure.caption.25}{}}
\newlabel{sub@fig:BD:epsilon}{{b}{9}{Time according to the epsilon value.\relax }{figure.caption.25}{}}
\newlabel{fig:BD:rate}{{11c}{9}{Time according to the the learning rate.\relax }{figure.caption.25}{}}
\newlabel{sub@fig:BD:rate}{{c}{9}{Time according to the the learning rate.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Number of iterations to converge to an optimal policy according to the parameters of the Grid World problem.\relax }}{9}{figure.caption.25}\protected@file@percent }
\newlabel{fig:BD:QL}{{11}{9}{Number of iterations to converge to an optimal policy according to the parameters of the Grid World problem.\relax }{figure.caption.25}{}}
\newlabel{fig:BD:qinit1}{{12a}{10}{Average reward for BD with initial Q value of 0.3.\relax }{figure.caption.26}{}}
\newlabel{sub@fig:BD:qinit1}{{a}{10}{Average reward for BD with initial Q value of 0.3.\relax }{figure.caption.26}{}}
\newlabel{fig:BD:qintit2}{{12b}{10}{Average reward for BD with initial Q value of 30.\relax }{figure.caption.26}{}}
\newlabel{sub@fig:BD:qintit2}{{b}{10}{Average reward for BD with initial Q value of 30.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Average reward for BD with different initial Q values.\relax }}{10}{figure.caption.26}\protected@file@percent }
\newlabel{fig:BD:aqinit}{{12}{10}{Average reward for BD with different initial Q values.\relax }{figure.caption.26}{}}
\newlabel{fig:BD:epsilon1}{{13a}{10}{Average reward for BD with epsilon value of 0.05.\relax }{figure.caption.27}{}}
\newlabel{sub@fig:BD:epsilon1}{{a}{10}{Average reward for BD with epsilon value of 0.05.\relax }{figure.caption.27}{}}
\newlabel{fig:BD:epsilon2}{{13b}{10}{Average reward for BD with epsilon value of 0.8.\relax }{figure.caption.27}{}}
\newlabel{sub@fig:BD:epsilon2}{{b}{10}{Average reward for BD with epsilon value of 0.8.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Average reward for BD with different initial Q values.\relax }}{10}{figure.caption.27}\protected@file@percent }
\newlabel{fig:BD:aepsilon}{{13}{10}{Average reward for BD with different initial Q values.\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{10}{section.5}\protected@file@percent }
\gdef \@abspage@last{10}
